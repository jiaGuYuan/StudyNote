特征工程:
    特征归一化:
        在实际应用中, 通过梯度下降法求解的模型通常是需要归一化的, 包括线性回归, 逻辑回归, 支持向量机, 神经网络等模型. 
        但对于决策树模型则并不适用, 以C4.5为例, 决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比, 
        而信息增益比跟特征是否经过归一化是无关的, 因为归一化并不会改变样本在特征x上的信息增益.
    类别类特征:
        除了决策树等少数模型能直接处理字符串形式的输入, 
        对于逻辑回归, 支持向量机等模型来说,类别型特征必须经过处理转换成数值型特征才能正确工作
    高维组合特征:
        广义矩阵分解. SVD,LFM,...
    组合特征:
        基于决策树的特征组合寻找方法,...
    文本表示模型:
        词袋模型(Bag of Words), TF-IDF(Term Frequency-Inverse Document Frequency), 主题模型(Topic Model), 词嵌入模型(Word Embedding)
    Word2Vec:
        深度学习模型正好为我们提供了一种自动地进行特征工程的方式, 模型中的每个隐层都可以认为对应着不同抽象层次的特征.
        LDA(主题模型).
        
        主题模型和词嵌入两类方法最大的不同其实在于模型本身, 主题模型是一种基于概率图模型的生成式模型, 
        其似然函数可以写成若干条件概率连乘的形式, 其中包括需要推测的隐含变量(即主题); 
        而词嵌入模型一般表达为神经网络的形式, 似然函数定义在网络的输出之上, 需要通过学习网络的权重以得到单词的稠密向量表示.
    图像数据不足时的处理方法:
        pass

模型评估:
    知道每种评估指标的精确定义, 有针对性地选择合适的评估指标, 根据评估指标的反馈进行模型调整
    分类问题:
        准确率(Accuracy), 
        精确率(Precision), 召回率(Recall) ===> P-R曲线(通过将阈值从高到低移动而生成的).
        F1 score
        ROC曲线, AUC
        
        P-R曲线 & ROC曲线:
            相比P-R曲线, ROC曲线有一个特点, 当正负样本的分布发生变化时, 
            ROC曲线的形状能够基本保持不变, 而P-R曲线的形状一般会发生较剧烈的变化.
            如果研究者希望更多地看到模型在特定数据集上的表现, P-R曲线则能够更直观地反映其性能
    排序问题:
    
    回归问题:
        均方根误差(Root Mean Square Error, RMSE) -- 对离群点(Outlier)很敏感
        平均绝对百分比误差(Mean AbsolutePercent Error, MAPE) --MAPE把每个点的误差进行了归一化,降低了个别离群点带来的绝对误差的影响
     
A/B Test:
    在充分的离线评估之后为什么还需要在线评估.
    
     
贝叶斯派思考问题的固定模式:
    先验分布f(a) + 样本信息x ==> 后验分布f(a|x)
    上述思考模式意味着,新观察到的样本信息将修正人们以前对事物的认知.
    换言之,在得到新的样本信息之前,人们对的认知是先验分布f(a),在得到新的样本信息x后,人们对的认知为f(a|x)

LDA(主题模型):
    主题分布(各个主题在文档中出现的概率分布)和词分布(各个词语在某个主题下出现的概率分布)

PCA & LDA:
    PCA(主成分分析):
        两种进行主成分分析的方法:
            1. 最大化投影方差(信号处理:信号具有较大方差, 噪声具有较小方差)
            2. 最小化数据点到超平面的距离平方和(线性回归:用一个低维的空间来拟合数据在原空间中的分布)
        PCA降维后会损失分类信息

    LDA(线性差别分析):
        为了分类服务的.降维的过程中不损失.
        LDA的中心思想——最大化类间距离和最小化类内距离.
        
    PCA vs LDA:
        在PCA中, 算法没有考虑数据的标签(类别),只是把原数据映射到一些方差比较大的方向上而已. 
        利用PCA进行降维,一般保留的是最佳描述特征(主成分), 而非分类特征. 
        如果我们想要保留数据的分类特征,应该用LDA方法对数据集进行降维.
        从应用的角度, 我们可以掌握一个基本的原则——对无监督的任务使用PCA进行降维, 对有监督的则应用LDA.
        
模型评估:
    准确率(Accuracy):
        Accuracy = Count(correct)/Count(total)
        --当不同类别的样本比例非常不均衡时,该评估指标倾向于占比大的类别
    查准率(Precision),召回率(Recall), 均方根误差(Root Mean Square Error,RMSE)

EM算法:
    EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题

    
训练集&测试集划分:
    Holdout检验: 
        随机划分, 训练集&测试集比例一般可选7:3
    交叉验证:
        k-fold交叉验证:
            将样本分成K等份,依次遍历这K个子集,将当前样本子集作为测试集,
            其他子集作为训练集.最后把k次评估指标的平均值作为最终的评估指标.
            在实际实验中, k经常取10.
    自助法(Bootstrap):
        当样本总数比较小时, 为避免将样本集进行划分使训练集进一步减小, 可使用自助采样法.
        对于总数为n的样本集合,进行n次有放回的随机抽样,得到大小为n的训练集.
        将没有被抽出的样本作为验证集,进行模型验证,这就是自助法的验证过程.
        n趋于无穷时,最终约有36.8%的数据从未被选择过.
    
超参数调优:
    目标函数,搜索范围,搜索步长
    
    网格搜索:
        一般先在较大的范围中用较大的步长来搜索,然后会逐渐缩小搜索范围和步长
    随机搜索:
        在搜索范围中随机采样
    贝叶斯优化:
        学习目标函数的形状: 先验分布(假设一个搜集函数) -->使用新的采样点测试目标函数,更新目标函数的先验分布-->由后验分布给出的全局最值最可能出现的位置点. 
    
**生成式模型 & 判别式模型**:
    生成式模型: 对联合概率分布p(x,c)进行建模,然后通过贝叶斯公式求p(c|x)
    判别式模型: 直接对条件概率p(c|x)进行建模
    
    
    
    
    
!!!一定要知道的:(接下来需要重点看的)
    SVM:
        SVM模型推导, 核函数, SMO(Sequential Minimal Optimization) 算法
        
    K-means CLustering, K-means++
    
    GMM(高斯混合模型):
    
    贝叶斯网络, 马尔可夫模型, 最大熵模型
    
    维特比算法
    
    决策树可不需要对数据进行特殊的预处理如归一化等
    
    Optimization:
        牛顿法(Newton"s method), 最小二乘法(Least Squares method), 梯度下降法(Gradient Descent)
    
        岭回归(Ridge Regression)
    
    多层感知机的平方误差和交叉熵损失函数的定义,并推导出各层参数更新的梯度计算公式
    
    
    集成学习:
        Boosting & Bagging的区别
        Boosting(Bootstrap Aggregating再抽样): 只能串行训练
        Bagging: 可以并行训练
            基于决策树基分类器的随机森林(Random Forest)
        Adaboost
        梯度提升决策树(GBDT)  --XGBoost是GBDT的一种实现
        
扩展:
    Softmax的改进:
        Hierarchical s oftmax (层次化softmax)
        negative softmax (负采样softmax)
            --word2vec中负采样时负样本是怎么来的??



KNN(K近邻)


SVD分解


为什么CART能做回归而ID3和C4.5不可以??
    cart有两种评价标准:Variance和Gini系数.而ID3和C4.5的评价基础都是信息熵.
    信息熵和Gini系数是针对分类任务的指标;而Variance是针对连续值的指标,因此可以用来做回归.
    --通过使用Variance作为评价标准, CART可以用于作回归.
      决策树最终将空间划分成多个区域,以区域内所有样本的均值来表示该区域的值.



