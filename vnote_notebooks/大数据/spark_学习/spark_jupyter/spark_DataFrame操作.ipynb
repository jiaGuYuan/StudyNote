{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = 'D:/Software/JDK_8U92'\n",
    "PYTHON_PATH = 'D:/Software/anaconda3/envs/wk_py37/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYTHON_PATH\n",
    "os.environ['PYSPARK_PYTHON'] = PYTHON_PATH\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, FloatType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").master('local[*]')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD与DataFrame之间的差别在于：<br/>\n",
    "* dataframe比rdd的速度快，对于结构化的数据，使用dataframe编写的代码更简洁。\n",
    "* 对于非结构化数据，建议先使用rdd处理成结构化数据，然后转换成dataframe。\n",
    "\n",
    "在dataFrame中的操作主要可以分为增,删,查,改,合并,统计,去重,格式转换,SQL操作,读写csv文件几个大的方向。<br/>\n",
    "我们从数据库中查询到的数据返回的数据结构就是DataFrame格式的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "datafile_dir = 'D:/StudyNote/原来的一些学习笔记_GJY/StudyNote/vnote_notebooks/大数据/spark_学习/test_data/'\n",
    "datafile1 = os.path.join(datafile_dir, 'WA_Fn-UseC_-Telco-Customer-Churn.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true')\\\n",
    "        .load(datafile1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema() # 以树的形式打印概要\n",
    "# df.dtypes # 查看数据类型\n",
    "# df.show(5) # 显示前N行\n",
    "\n",
    "# df[\"Partner\", \"gender\"].describe().show() # 对df中的数据进行统计，返回常用的一些统计指标的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3) # df.take(5) # 获取前N行数据,以python列表形式返回\n",
    "\n",
    "df.count() # 查询总行数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column.alias(*alias) 用列别名返回\n",
    "用一个或多个新名称返回此列(对于返回多个列的表达式,如explode)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.customerID.alias(\"customerID_alias\")).show(2) # 为查询的列取别名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('gender', 'Partner').count() # 计数\n",
    "df.select('gender', 'Partner').distinct().count() # 去重后计数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df.sample()随机取样\n",
    "```\n",
    "df.sample(withReplacement=None, fraction=None, seed=None)\n",
    "withReplacement: 是否进行有放回的抽样, 默认False\n",
    "fraction: 抽样比例fraction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(False, 0.5, 0)\n",
    "sample.count() # 随机取样，抽取50%的数据出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expr(str)将字符串作为一个函数来执行\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "expr(str)将表达式字符串解析为它所表示的列,或对列的函数操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.select(expr('Churn')).show(5) # 返回'Churn'列取值的长度\n",
    "\n",
    "df.select(expr('length(Churn)')).show(5) # 返回'Churn'列取值的长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select() 选取某一列或某几列数据\n",
    "select返回的是dataframe格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.select(\"customerID\") \n",
    "df.select(df.customerID, df.gender)\n",
    "df.select(df[\"customerID\"], df[\"gender\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df.where(condition) 选择满足条件的数据\n",
    "与df.filter()功能一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.gender==\"Female\").where(df.Churn>='Yes').count()\n",
    "df.where((df.gender==\"Female\") & (df.Churn>='Yes')).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其它\n",
    "```\n",
    "withColumnRenamed(\"old_name\", \"new_name\") # 重命名列\n",
    "asc()  # 基于指定列返回一个升序表达式\n",
    "desc()  # 基于指定列返回一个降序表达式\n",
    "astype(dataType)   # 转换列的数据类型,跟cast()是同一个函数\n",
    "cast(dataType)   # 转换数据类型\n",
    "startswith(string)  # 判断列中每个值是否以指定字符开头，返回布尔值\n",
    "endswith(string)   # 判断列中每个值是否以指定字符结尾，返回布尔值\n",
    "isNotNull()   # 判断列中的值是否不为空\n",
    "isNull()   # 判断列中的值是否为空\n",
    "like(expression)   # 判断列中的值是否满足相似条件,判断条件跟sql语法相同,支持通配符\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 窗口函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df.select(\"customerID\", \n",
    "          F.rank().over(Window.orderBy(\"customerID\")).alias(\"customerID_rank\"))\\\n",
    "      .show(5)\n",
    "\n",
    "\n",
    "df.select(\"customerID\", \n",
    "          F.rank().over(Window.orderBy(df.customerID.desc())).alias(\"customerID_rank\"))\\\n",
    "      .show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按条件筛选when / between\n",
    "`when(condition1, value1).when(condition2, value2)..otherwise(other_value)`<br/>\n",
    "当满足条件condition1时赋值为values1,当满足条件condition2时赋值为values2,....条件都不满足时赋值other_value.<br/>\n",
    "otherwise表示,不满足条件的情况下，应该赋值何值.<br/>\n",
    "\n",
    "与SQL中的case...when...语句类似."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.select(df.customerID, \n",
    "          F.when(df.gender==\"Male\", \"1\")\\\n",
    "           .when(df.gender==\"Female\", \"0\")\\\n",
    "           .otherwise(\"2\")\\\n",
    "           .alias(\"sex\"))\\\n",
    "    .show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## between(lowerBound, upperBound) # 筛选出某个范围内的值\n",
    "返回的是TRUE or FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.customerID, df.tenure.between(10, 20).alias(\"tenure\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果需要进行筛选df中的特定区间内的数据时，可以使用下面这种通过行索引进行筛选的方式\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dfWithIndex = df.withColumn(\"index\", monotonically_increasing_id())  # 为df添加行索引\n",
    "dfWithIndex.select(dfWithIndex.index, dfWithIndex.tenure, \n",
    "                   dfWithIndex.tenure.between(\"30\",\"50\").alias(\"tenure\"))\\\n",
    "    .show(5) #筛选特定行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter()过滤数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(df['tenure']>=21) # 等价于 df = df.where(df['tenure']>=21)\n",
    "df2.count()\n",
    "\n",
    "df2 = df.filter(\"tenure>=21\") # 可以用sql字符串的方式指定条件\n",
    "df2.count()\n",
    "\n",
    "# 同时指定多个过滤条件\n",
    "df2 = df .filter(\"tenure>=21 and tenure<=30\") # \n",
    "df2.count()\n",
    "\n",
    "# 过滤null值或nan值时：\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "df3 = df.filter(isnull(\"tenure\")) # 把列中数据为null的筛选出来(代表python的None类型)\n",
    "df3.count()\n",
    "\n",
    "df3 = df.filter(isnan(\"tenure\"))  # 把列中数据为nan的筛选出来(Not a Number,非数字数据)\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas的DataFrame <====> pyspark的DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame({\"name\":[\"ss\",\"aa\",\"qq\",\"ee\"],\n",
    "                          \"age\":[12,18,20,25]})\n",
    "\n",
    "spark_df = spark.createDataFrame(pandas_df) # pandas中的df ==> pyspark中的df\n",
    "type(spark_df)\n",
    "\n",
    "new_pandas = spark_df.toPandas() # pyspark中的df ==> pandas中的df\n",
    "type(new_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将rdd转为spark中的DataFrame格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(\"spe_id\", \"InOther\")\n",
    "x = ['x1','x2']\n",
    "y = ['y1','y2']\n",
    "new_df = sc.parallelize([row(x[i], y[i]) for i in range(2)]) # RDD\n",
    "spark_df = new_df.toDF() # RDD ==> DataFrame\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumn()\n",
    "通过添加列或替换具有相同名称的现有列,来得到一个新的DataFrame.<br/>\n",
    "熟用pyspark.sql.functions.<br/>\n",
    "```\n",
    "from pyspark.sql import functions as F\n",
    "F.lit('xxx') # 创建一个字面值常量的列.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.select(\"customerID\",\"gender\",\"Partner\",\"tenure\")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 返回一个新的DF, 通过添加newcol_gender列(由gender列转换成大写)\n",
    "withcolumns_df = df1.withColumn(\"newcol_gender\", F.upper('gender')) \n",
    "withcolumns_df.show(3)\n",
    "\n",
    "# 返回一个新的DF, 通过替换tenure列(取值+1)\n",
    "withcolumns_df = df1.withColumn(\"tenure\", df.tenure+1)\n",
    "withcolumns_df.show(3)\n",
    "\n",
    "# 修改指定列的类型\n",
    "df1.printSchema()\n",
    "df2 = df1.withColumn('tenure', F.col('tenure').cast(\"bigint\"))\n",
    "df2.printSchema()\n",
    "\n",
    "# 转换列可通过多个字段进行\n",
    "df2 = df1.withColumn('new_col', \n",
    "                F.concat_ws('_', *[F.lit(\"gender\"), F.lit(\"Partner\")]))\n",
    "df2.show(3)\n",
    "\n",
    "# 自定义UDF函数\n",
    "def udf_concat_ws(sep, *cols):\n",
    "    return sep.join(cols)\n",
    "# 创建一个用户自定义的UDF函数(使用时只需要传入列名即可,函数内部自动得到列的值)\n",
    "f_udf_concat_ws = F.udf(udf_concat_ws, StringType()) \n",
    "\n",
    "df2 = df1.withColumn('new_col', f_udf_concat_ws(F.lit('_'), \"gender\", \"Partner\")) # 注意F.lit()的使用\n",
    "df2.show(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## union()合并的操作(与SQL union all类似)\n",
    "行合并(行会增加),\n",
    "要求合并的两端具有相同的列名及列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select(\"customerID\", \"gender\", \"tenure\").filter(\"tenure=1\")\n",
    "df3 = df.select(\"customerID\", \"gender\", \"tenure\").filter(\"tenure=2\")\n",
    "df4 = df2.union(df3) # 等价于unionAll()\n",
    "df2.count()\n",
    "df3.count()\n",
    "df4.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join()根据条件进行关联 (与SQL join类似)\n",
    "```\n",
    "df1.join(df2, on=None, how=None)\n",
    "参数说明:\n",
    "on: 联结条件, 可以是\n",
    "    1. 要联结的列名字符串(单字段联结),eg: 'colName' \n",
    "    2. 要联结的列名字符串列表(多字段联结), eg: ['colName1', 'colName2']\n",
    "    3. 关于Column的联结表达式(单字段联结), eg: df1.colnameX==df2.colnameY \n",
    "    4. 关于Column的联结表达式列表(多字段联结), eg: [df1.colnameX1==df2.colnameX2, df1.colnameY1==df2.colnameY2]\n",
    "    其中1/2 要求联结列在df1,df2中同名; 3/4 不要求联结列在df1,df2中同名.\n",
    "``` \n",
    "    \n",
    "    \n",
    "列合并(列会增加). <br/>\n",
    "注意:使用df=df1.join(df2, on=)进行关联后,在结果df中可能会存在同名的列.<br/>\n",
    "可在连接后使用select()选择出需要的列,或重新命名. <br/>\n",
    "eg: df=df1.join(df2, on=).select(df1.colname1, df1.colname2, df2.colnamex.alias('colname_alias'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.select(\"customerID\", \"gender\", \"tenure\")\n",
    "df3 = df1.select(\"gender\", \"tenure\", \"Partner\")\n",
    "df4 = df2.join(df3, df1.gender==df2.gender, how=\"left_outer\")\n",
    "# df4 = df2.join(df3, 'gender', how=\"left_outer\")\n",
    "df4 = df4.select(df2.customerID, df2.gender, df2.tenure, df3.tenure.alias('tenure_2'))\n",
    "df4.show(5)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 差集,交集,并集\n",
    "* df1.subtract(df2)求差集\n",
    "返回一个新的df, 其由在df1中但不在df2中的行组成\n",
    "\n",
    "* df1.intersect(df2)求交集\n",
    "返回一个新的df, 其由在df1中又在df2中的行组成\n",
    "\n",
    "* df1.union(df2) 求并集(但并不去重). union+distinct求并集并进行去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame((\n",
    "      (1, \"asf\"),\n",
    "      (2, \"2143\"),\n",
    "      (3, \"rfds\")\n",
    "    )).toDF(\"label\", \"sentence\")\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame((\n",
    "      (1, \"asf\"),\n",
    "      (2, \"2143\"),\n",
    "      (4, \"f8934y\")\n",
    "    )).toDF(\"label\", \"sentence\")\n",
    "\n",
    "# 差集\n",
    "new_df = df1.select(\"sentence\").subtract(df2.select(\"sentence\"))\n",
    "new_df.show()\n",
    "\n",
    "# 交集\n",
    "new_df2 = df1.select(\"sentence\").intersect(df2.select(\"sentence\"))\n",
    "new_df2.show()\n",
    "\n",
    "# 并集\n",
    "new_df3 = df1.select(\"sentence\").union(df2.select(\"sentence\"))\n",
    "new_df3.show()\n",
    "\n",
    "# 求并集并去重\n",
    "new_df4 = df1.select(\"sentence\").union(df2.select(\"sentence\")).distinct()\n",
    "new_df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行 <==> 列 \n",
    "行列转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将一行数据拆成多行(列转行)\n",
    "explode()+ split() # 对某列的数据以固定的分隔符进行分隔,并新增一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df1 = spark.createDataFrame((\n",
    "      (1, \"XXX-YYY-ZZZ\"),\n",
    "      (2, \"AAA BB\"),\n",
    "      (3, \"CC-DD\")\n",
    "    )).toDF(\"label\", \"sentence\")\n",
    "df1.show()\n",
    "\n",
    "# 将sentence列值按'-'分隔并扩展成多行\n",
    "df1.withColumn(\"sentence_d\", F.explode(F.split(df1.sentence, \"-\"))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pivot()聚合透视(行转列)\n",
    "\n",
    "执行指定的聚合并对对指定列进行透视。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = spark.sparkContext.parallelize(\n",
    "    [[15,11,2],\n",
    "     [15,14,5],\n",
    "     [15,16,4],\n",
    "     [15,13,4], \n",
    "     [18,21,3],\n",
    "     [18,11,3],\n",
    "     [18,13,1]]\n",
    "    ).toDF([\"userID\",\"movieID\",\"rating\"])\n",
    "dfx.show()\n",
    "\n",
    "# pivot 透视\n",
    "res_df = dfx.groupBy(\"userID\").pivot(\"movieID\").sum(\"rating\")\n",
    "res_df.show()\n",
    "\n",
    "\n",
    "# 手动指定要透视键的取值列表 -- 效率要高于不指定时\n",
    "res_df = dfx.groupBy(\"userID\")\\\n",
    "            .pivot(\"movieID\", values=[11,13,14,16,21])\\\n",
    "            .sum(\"rating\")\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 频数统计与筛选，根据tenure与gender字段，统计该字段值出现频率在40%以上的内容\n",
    "df.stat.freqItems([\"tenure\", \"gender\"], 0.4).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉分析\n",
    "df.crosstab('tenure', 'Partner').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分组函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('gender').agg({'tenure': 'mean'}).show(5)\n",
    "\n",
    "df.groupby('gender').count().show()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "df.groupBy(\"gender\")\\\n",
    "   .agg(F.avg(\"tenure\"), F.min(\"tenure\"), F.max(\"tenure\"))\\\n",
    "   .show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除的操作\n",
    "df = df.drop(*cols) -- 删除指定的一列或或多列<br/>\n",
    "df = df.na.drop(how='any', subset=[col1, col2, ...]) # 如果一行中指定列包含null值则删除<br/>\n",
    "df = df.dropna() 等价于 df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = spark.createDataFrame((\n",
    "      (1, \"asf\", 11),\n",
    "      (2, \"2143\", 21),\n",
    "      (3, \"rfds\", 31)\n",
    "    )).toDF(\"label\", \"sentence\", 'xx')\n",
    "df1.show()\n",
    "\n",
    "df1.drop(*[\"label\", \"xx\"]).columns\n",
    "df1.drop(df1.label).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame((\n",
    "      (1, None, 11),\n",
    "      (2, \"2143\", None),\n",
    "      (3, \"rfds\", 31),\n",
    "      (4, None, None)\n",
    "    )).toDF(\"label\", \"sentence\", 'xx')\n",
    "df1.show()\n",
    "\n",
    "df1.na.drop(how='any', subset=['sentence', 'xx']).show() # 如果一行中指定列包含null值则删除\n",
    "df1.na.drop(how='all', subset=['sentence', 'xx']).show() # 如果一行中指定列全是null值则删除\n",
    "# how:默认为any, subset:默认为None,即所有列\n",
    "\n",
    "\n",
    "# df.dropna() 等价于 df.na.drop()\n",
    "df1.dropna(how='any', subset=['sentence', 'xx']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 空值填充fillna() \n",
    "df.na.fill()与fillna()相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame((\n",
    "      (1, None, 11),\n",
    "      (2, \"2143\", None),\n",
    "      (3, \"rfds\", 31),\n",
    "      (4, None, None)\n",
    "    )).toDF(\"label\", \"sentence\", 'xx')\n",
    "df1.show()\n",
    "\n",
    "df1.fillna(-1).show() # -1填充\n",
    "\n",
    "means = df1.select(F.avg(df1.xx))\n",
    "df1.na.fill({\"xx\": means.toPandas().values[0][0]}).show() # 均值填充\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去重的操作\n",
    "df1.dropDuplicates(subset=['sentence']).show() # 根据指定列去重. 类似于select distinct a, b操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame((\n",
    "      (1, \"asf\", 11),\n",
    "      (2, \"hello\", 21),\n",
    "      (3, \"rfds\", 31),\n",
    "      (4, \"hello\", 31)\n",
    "    )).toDF(\"label\", \"sentence\", 'xx')\n",
    "df1.show()\n",
    "\n",
    "\n",
    "df1.dropDuplicates(subset=['sentence']).show() # 根据指定列去重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 从hive读取数据,要在开始的时候创建一个sparksession的对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "myspark = SparkSession.builder \\\n",
    "    .appName('compute_customer_age') \\\n",
    "    .config('spark.executor.memory','2g') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 pyspark.sql.functions.udf 进行自定义函数的使用\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df1 = spark.createDataFrame((\n",
    "      (1, \"asf\", 11),\n",
    "      (2, \"hello\", 21),\n",
    "      (3, \"rfds\", 31),\n",
    "      (4, \"hello\", 31)\n",
    "    )).toDF(\"label\", \"sentence\", 'xx')\n",
    "df1.show()\n",
    "\n",
    "\n",
    "def udf_test(x):\n",
    "    if x >= 30:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return \"no\"\n",
    "f_udf_test = udf(udf_test, returnType=StringType())   # 注册函数\n",
    "\n",
    "df2 = df1.withColumn(\"xx_new\", f_udf_test(df1.xx))   # 调用函数\n",
    "df2.show()\n",
    "\n",
    "# 使用udf对性能会有负面的影响，如果不是太过于复杂的逻辑\n",
    "# 可以使用f.when.when.otherwise()的方式得出想要的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字符串方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符串方法\n",
    "\n",
    "from pyspark.sql.functions import concat, concat_ws\n",
    "df1 = spark.createDataFrame([('abcd', '123')], ['s', 'd'])\n",
    "df1.show()\n",
    "\n",
    "# 直接拼接\n",
    "df1.select(concat(df1.s, df1.d).alias('s_d')).show()\n",
    "\n",
    "# 指定拼接符\n",
    "df1.select(concat_ws('-', df1.s, df1.d).alias('s-d')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格式化字符串\n",
    "from pyspark.sql.functions import format_string\n",
    "df = spark.createDataFrame([(5, \"hello\")], ['col_a', 'col_b'])\n",
    "df.select(format_string('%d %s', df.col_a, df.col_b).alias('col_v'))\\\n",
    "  .withColumnRenamed(\"col_av\", \"col_vv\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找字符串的位置\n",
    "from pyspark.sql.functions import instr\n",
    "df = spark.createDataFrame([('abcd',)], ['col_s'])\n",
    "df.show()\n",
    "df.select(instr(df.col_s, 'b').alias('col_s_idx')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符串截取\n",
    "from pyspark.sql.functions import substring\n",
    "df = spark.createDataFrame([('abcd',)], ['col_s'])\n",
    "df.select(substring(df.col_s, 1, 2).alias('col_sub')).show()  #1与2表示开始与截取长度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则表达式替换\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "df = spark.createDataFrame([('100sss200',)], ['col_str'])\n",
    "df.select(regexp_replace('col_str', '(\\d)', '-').alias('col_regex')).collect()  # 替换类型，正则语句，替换内容\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与时间有关的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与时间有关的方法\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['col_dt'])\n",
    "df.select(F.date_format('col_dt', 'yyyyMMdd').alias('date')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "spark.range(3).withColumn('date', current_date()).show() # 获取当前日期\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "spark.range(3).withColumn('date', current_timestamp()).show() # 获取当前时间戳\n",
    "\n",
    "\n",
    "# 字符串日期 ==> 时间日期格式\n",
    "\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['col_dt'])\n",
    "df.select(to_date(df.col_dt).alias('date')).show()   # 转日期\n",
    "df.select(to_timestamp(df.col_dt).alias('dt')).show()   # 带时间的日期\n",
    "df.select(to_timestamp(df.col_dt, 'yyyy-MM-dd HH:mm:ss').alias('dt')).show()   # 可以指定日期格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取日期中的年月日\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['col_dt'])\n",
    "df.select(year('col_dt').alias('year'), \n",
    "          month('col_dt').alias('month'),\n",
    "          dayofmonth('col_dt').alias('day')\n",
    "  ).show()\n",
    "\n",
    "# 只接受以-连接的日期，如果是以/等其他连接符连接的，需要进行格式转换或者字符替换。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import datediff, months_between \n",
    "\n",
    "# 日期差\n",
    "df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    "df.select(datediff(df.d2, df.d1).alias('diff')).show() \n",
    "\n",
    "# 月份差\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['t', 'd'])\n",
    "df.select(months_between(df.t, df.d).alias('months')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions其它函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.functions.abs(col)   # 计算绝对值\n",
    "pyspark.sql.functions.avg(col)   # 聚合函数：返回组中的值的平均值。\n",
    "pyspark.sql.functions.variance(col)   # 返回组中值的总体方差\n",
    "pyspark.sql.functions.ceil(col)   # 计算给定值的上限\n",
    "pyspark.sql.functions.floor(col)   # 计算给定值的下限。\n",
    "pyspark.sql.functions.collect_list(col)   # 返回重复对象的列表。\n",
    "pyspark.sql.functions.collect_set(col)    # 返回一组消除重复元素的对象。\n",
    "pyspark.sql.functions.count(col)    # 返回组中的项数量。\n",
    "pyspark.sql.functions.countDistinct(col, *cols)   # 返回一列或多列的去重计数的新列。\n",
    "pyspark.sql.functions.initcap(col)    # 在句子中将每个单词的第一个字母翻译成大写。\n",
    "pyspark.sql.functions.isnan(col)    # 如果列是NaN，则返回true的表达式\n",
    "pyspark.sql.functions.lit(col)     # 创建一个文字值的列\n",
    "pyspark.sql.functions.lower(col)   # 将字符串列转换为小写\n",
    "pyspark.sql.functions.reverse(col)   # 反转字符串列并将其作为新的字符串列返回\n",
    "pyspark.sql.functions.sort_array(col, asc=True)   # 按升序对给定列的输入数组进行排序\n",
    "pyspark.sql.functions.split(str, pattern)   # 按指定字符进行分隔数据\n",
    "pyspark.sql.functions. array_min (col)   # 计算指定列的最小值\n",
    "pyspark.sql.functions. array_max (col)   # 计算指定列的最大值\n",
    "pyspark.sql.functions.stddev(col)    # 返回组中表达式的无偏样本标准差\n",
    "pyspark.sql.functions.sumDistinct(col)   # 返回表达式中不同值的总和\n",
    "pyspark.sql.functions.trim(col)   # 去除空格\n",
    "pyspark.sql.functions. greatest (col1,col2)   # 求行的最大值，可以计算一行中多列的最大值\n",
    "pyspark.sql.functions. least (col1,col2)   # 求行的最小值，可以计算一行中多列的最小值，也可以用lit()指定常数进行与列的值进行比较\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wk_py37",
   "language": "python",
   "name": "env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221.009px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
