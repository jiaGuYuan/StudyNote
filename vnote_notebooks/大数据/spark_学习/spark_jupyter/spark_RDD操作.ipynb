{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = 'D:/Software/JDK_8U92'\n",
    "PYTHON_PATH = 'D:/Software/anaconda3/envs/wk_py37/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYTHON_PATH\n",
    "os.environ['PYSPARK_PYTHON'] = PYTHON_PATH\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, FloatType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").master('local[*]')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = sc.parallelize(\n",
    "    [\"scala\",\n",
    "     \"java\",\n",
    "     \"hadoop\",\n",
    "     \"spark\",\n",
    "     \"akka\",\n",
    "     \"spark vs hadoop\",\n",
    "     \"pyspark\",\n",
    "     \"pyspark and spark\"\n",
    "     ])\n",
    "counts = words.count()\n",
    "print(f\"Number of elements in RDD -> {counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "datafile_dir = 'D:/StudyNote/原来的一些学习笔记_GJY/StudyNote/vnote_notebooks/大数据/spark_学习/test_data/'\n",
    "datafile1 = os.path.join(datafile_dir, 'data1.txt')\n",
    "\n",
    "dataflie1 = sc.textFile(datafile1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflie1\n",
    "dataflie1.collect()\n",
    "print(dataflie1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd = sc.textFile(logfile_path) \n",
    "data_rdd.collect()\n",
    "data_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/D:/StudyNote/原来的一些学习笔记_GJY/StudyNote/vnote_notebooks/大数据/spark_学习/test_data/data1.txt',\n",
       "  'comboid subproduct_idx combo_subproduct_id is_recursive_combo subproduct_purchase_price combo_subproduct_num combo_purchase_price_rate sub_combo_flag is_entity_combo\\r\\n19534678\\t0\\t56056554\\t0\\t12.38\\t2\\t0.5\\t0\\t0\\r\\n19534679\\t0\\t77021798\\t0\\t27.18\\t3\\t0.33333333333333337\\t0\\t0\\r\\n19534680\\t0\\t60033648\\t0\\t19.93\\t1\\t0.912963811268896\\t0\\t0\\r\\n19534680\\t1\\t79018735\\t0\\t1.9\\t1\\t0.08703618873110398\\t0\\t0\\r\\n19534681\\t0\\t60033649\\t0\\t19.93\\t1\\t0.912963811268896\\t0\\t0\\r\\n19534681\\t1\\t79018735\\t0\\t1.9\\t1\\t0.08703618873110398\\t0\\t0\\r\\n19534682\\t0\\t60033650\\t0\\t19.93\\t1\\t0.912963811268896\\t0\\t0\\r\\n19534682\\t1\\t79018735\\t0\\t1.9\\t1\\t0.08703618873110398\\t0\\t0\\r\\n19534683\\t0\\t78016297\\t0\\t19.38\\t1\\t0.4640804597701149\\t0\\t0\\r\\n19534683\\t1\\t78017178\\t0\\t22.38\\t1\\t0.5359195402298851\\t0\\t0\\r\\n19534684\\t0\\t62030541\\t0\\t14.38\\t2\\t0.5\\t0\\t0\\r\\n19534685\\t0\\t62030547\\t0\\t18.38\\t1\\t0.5\\t0\\t0\\r\\n19534685\\t1\\t62030548\\t0\\t18.38\\t1\\t0.5\\t0\\t0\\r\\n19534686\\t0\\t62030546\\t0\\t26.38\\t2\\t0.5\\t0\\t0'),\n",
       " ('file:/D:/StudyNote/原来的一些学习笔记_GJY/StudyNote/vnote_notebooks/大数据/spark_学习/test_data/data2.txt',\n",
       "  'comboid subproduct_idx combo_subproduct_id is_recursive_combo subproduct_purchase_price combo_subproduct_num combo_purchase_price_rate sub_combo_flag is_entity_combo\\r\\n19534701\\t0\\t78016799\\t0\\t19.38\\t1\\t0.5\\t0\\t0\\r\\n19534701\\t1\\t78016803\\t0\\t19.38\\t1\\t0.5\\t0\\t0\\r\\n19534702\\t0\\t61034628\\t0\\t18.38\\t1\\t0.45093228655544654\\t0\\t0\\r\\n19534702\\t1\\t61034630\\t0\\t22.38\\t1\\t0.5490677134445535\\t0\\t0\\r\\n19534703\\t0\\t57048544\\t0\\t4.18\\t1\\t0.5\\t0\\t0\\r\\n19534703\\t1\\t57048545\\t0\\t4.18\\t1\\t0.5\\t0\\t0\\r\\n19534704\\t0\\t61034628\\t0\\t18.38\\t1\\t0.420018281535649\\t0\\t0\\r\\n19534704\\t1\\t61034629\\t0\\t25.38\\t1\\t0.579981718464351\\t0\\t0\\r\\n19534705\\t0\\t57047364\\t0\\t6.58\\t1\\t0.5\\t0\\t0\\r\\n19534705\\t1\\t57047368\\t0\\t6.58\\t1\\t0.5\\t0\\t0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile_dir_pattern = os.path.join(datafile_dir, '*.txt') \n",
    "path_data_rdd = sc.wholeTextFiles(datafile_dir_pattern)\n",
    "path_data_rdd.collect()\n",
    "len(path_data_rdd.collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.wholeTextFiles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## foreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize(\n",
    "    [\"scala\",\n",
    "     \"java\",\n",
    "     \"hadoop\",\n",
    "     \"spark\",\n",
    "     \"akka\",\n",
    "     \"spark vs hadoop\",\n",
    "     \"pyspark\",\n",
    "     \"pyspark and spark\"\n",
    "     ])\n",
    "\n",
    "def f_foreach(x):\n",
    "    print(x)\n",
    "\n",
    "words.foreach(f_foreach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.filter(f)\n",
    "只取出满足条件的元素, 返回一个新RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize(\n",
    "    [\"scala\",\n",
    "     \"java\",\n",
    "     \"hadoop\",\n",
    "     \"spark\",\n",
    "     \"akka\",\n",
    "     \"spark vs hadoop\",\n",
    "     \"pyspark\",\n",
    "     \"pyspark and spark\"\n",
    "     ])\n",
    "\n",
    "\n",
    "rdd2 = words.filter(lambda x: 'spark' in x)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.map(f)\n",
    "一对一转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize(\n",
    "    [\"scala\",\n",
    "     \"java\",\n",
    "     \"hadoop\",\n",
    "     \"spark\",\n",
    "     \"akka\",\n",
    "     \"spark vs hadoop\",\n",
    "     \"pyspark\",\n",
    "     \"pyspark and spark\"\n",
    "     ])\n",
    "\n",
    "\n",
    "rdd2 = words.map(lambda x: (x, 1))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.reduce(f)\n",
    "使用指定的交换和结合二元运算符 减少这个RDD的元素。减小了本地分区.\n",
    "与python的reduce一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print(f\"Adding all the elements -> {adding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.join(other, numPartitions=None)\n",
    "返回一个RDD，包含在'self'和'other'中具有匹配键的所有对元素。<br/>\n",
    "rdd,other的元素都被视作是二元的 k-v pair. <br/>\n",
    "当rdd,other元素的长度超过2个时,元素第一个值被视为k,第二个值被视作v,其他值将被忽略."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "rdd2 = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = rdd1.join(rdd2)\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.distinct()\n",
    "去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intRDD = sc.parallelize([3,1,2,5,5])\n",
    "intRDD.distinct().collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.randomSplit([0.2, 0.3, ...])\n",
    "将整个集合以随机的方式按照比例分为多个RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intRDD = sc.parallelize([3,1,2,5,5, 0, 1,2,3,4,6])\n",
    "sRDD = intRDD.randomSplit([0.4,0.6])\n",
    "len(sRDD)\n",
    "sRDD[0].collect()\n",
    "sRDD[1].collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.groupBy(f)\n",
    "按照传入函数的规则，将数据分为多个Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intRDD = sc.parallelize([3,1,2,5,5, 0, 1,2,3,4,6])\n",
    "result = intRDD.groupBy(lambda x : x % 2).collect() # 分成2个组\n",
    "result\n",
    "\n",
    "\n",
    "result2 = intRDD.groupBy(lambda x : x % 3).collect() # 分成3个组\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.union(other)\n",
    "并集运算,但不会去重(效果像sql 的 union all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intRDD1 = sc.parallelize([3,1,2,5,5])\n",
    "intRDD2 = sc.parallelize([5,6])\n",
    "intRDD3 = sc.parallelize([2,7])\n",
    "intRDD1.union(intRDD2).union(intRDD3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.intersection(other)\n",
    "交集运算,结果中无重复元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.subtract(other) \n",
    "差集运算(返回在\"rdd\"中,但不包含在\"other\"中的每个元素)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "x.subtract(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rdd.cartesian(other)\n",
    "进行笛卡尔乘积运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "x.cartesian(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD基本动作运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intRDD = sc.parallelize([3,1,2,5,5, 0, 1,2,3,4,6])\n",
    "#取第一条数据\n",
    "intRDD.first()\n",
    "\n",
    "#取前两条数据\n",
    "intRDD.take(2)\n",
    "\n",
    "# takeOrdered方法应该只在结果数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中.\n",
    "# rdd.takeOrdered(num, key=None) -- 按照key操作对元素的评分从小到大排序\n",
    "\n",
    "#升序排列，并取前3条数据\n",
    "intRDD.takeOrdered(3)\n",
    "\n",
    "#降序排列，并取前3条数据\n",
    "intRDD.takeOrdered(3, lambda x:-x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计功能\n",
    "可以将RDD内的元素进行统计运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intRDD = sc.parallelize([3,1,2,5,5, 0, 1,2,3,4,6])\n",
    "\n",
    "#统计  会返回一些常见的统计指标的值\n",
    "intRDD.stats()\n",
    "\n",
    "#最小值\n",
    "intRDD.min()\n",
    "#最大值\n",
    "intRDD.max()\n",
    "#标准差\n",
    "intRDD.stdev()\n",
    "#计数\n",
    "intRDD.count()\n",
    "#求和\n",
    "intRDD.sum()\n",
    "#平均\n",
    "intRDD.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Key-Value基本\"转换\"运算\n",
    "\n",
    "sc.parallelize([(k, v), (k, v),...]) <br/>\n",
    "我们用元素类型为tuple的数组初始化我们的RDD; 这里,每个tuple的第一个值将作为键,而第二个元素将作为值。<br/>\n",
    "可以使用keys和values函数分别得到RDD的键数组和值数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvRDD1 = sc.parallelize([(3,4), (3,6), (5,6), (1,2)])\n",
    "\n",
    "kvRDD1.keys().collect()\n",
    "kvRDD1.values().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用filter函数筛选元素,可以按照键进行元素筛选，也可以通过值进行元素筛选.\n",
    "kvRDD1 = sc.parallelize([(3,4), (3,6), (5,6), (1,2)])\n",
    "\n",
    "# 按键进行筛选 得到一个新的RDD\n",
    "kvRDD1.filter(lambda x:x[0] < 5).collect()\n",
    "\n",
    "# 按值进行筛选 得到一个新的RDD\n",
    "kvRDD1.filter(lambda x:x[1] < 5).collect()\n",
    "\n",
    "\n",
    "# 对值进行处理得到一个新的RDD\n",
    "kvRDD1.mapValues(lambda x:x**2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kvRdd.sortByKey()对rdd按照key进行排序(rdd由(key, value)对组成)\n",
    "```\n",
    "kvRdd.sortByKey(\n",
    "    ascending=True,\n",
    "    numPartitions=None,\n",
    "    keyfunc=?,\n",
    ")\n",
    "```\n",
    "rdd由(key, value)对组成.\n",
    "对这个RDD的key进行按照keyfunc函数进行评分,并对评分按照ascending指定的顺序进行排序.\n",
    "未设置时keyfunc时,评分即为key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvRDD1 = sc.parallelize([(3,4), (3,6), (5,6), (1,2)])\n",
    "kvRDD1.sortByKey().collect() # 对key升序\n",
    "kvRDD1.sortByKey(False).collect() # 对key降序\n",
    "\n",
    "kvRDD1.sortByKey(ascending=True, keyfunc=lambda k: -k).collect() # 对-k升序\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kvRdd.reduceByKey()对具有相同key值的数据进行合并\n",
    "```\n",
    "kvRdd.reduceByKey(\n",
    "    func,\n",
    "    numPartitions=None,\n",
    "    partitionFunc=?,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join内连接运算\n",
    "类似数据库的内连接\n",
    "\n",
    "`kvRdd1.join(kvRdd2, numPartitions=None)` <br/>\n",
    "返回一个新的RDD,其每个元素格式为(k,(v1, v2)).<br/>\n",
    "其中(k, v1)在kvRdd1中, (k, v2)在kvRdd2中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])\n",
    "kvRDD2 = sc.parallelize([(3,8), (3,1)])\n",
    "kvRDD1.join(kvRDD2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leftOuterJoin左外连接\n",
    "类似数据库的左外连接\n",
    "`kvRdd1.leftOuterJoin(kvRdd2, numPartitions=None)`<br/>\n",
    "\n",
    "rightOuterJoin右外连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "x.leftOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kvRdd1.subtractByKey(kvRdd2)返回key在vkRdd1但不在kvRdd2中的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])\n",
    "kvRDD2 = sc.parallelize([(3,8), (1,2)])\n",
    "kvRDD1.subtractByKey(kvRDD2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kvRdd.countByKey()函数可以统计各个key值对应的数据的条数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])\n",
    "kvRDD1.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kvRdd.lookup(key)函数可以根据输入的key值来查找对应的Value值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])\n",
    "kvRDD1.lookup(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 持久化操作\n",
    "spark RDD的持久化机制，可以将需要重复运算的RDD存储在内存中，以便大幅提升运算效率.<br/>\n",
    "有两个主要的函数：\n",
    "   * persist() :对RDD进行持久化\n",
    "   * unpersist() : 对RDD进行去持久化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wk_py37",
   "language": "python",
   "name": "env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
